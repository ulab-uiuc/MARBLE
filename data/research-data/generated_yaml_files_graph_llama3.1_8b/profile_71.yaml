agents:
- agent_id: agent1
  profile: 'I am a researcher dedicated to advancing the field of multilingual natural
    language processing (NLP) and enhancing the capabilities of language models, particularly
    for low-resource languages. My recent work includes the development of self-translate,
    a novel approach that leverages the few-shot translation capabilities of multilingual
    models, demonstrating significant improvements over traditional translation methods.
    I also expanded the XNLI benchmark to include Basque, creating the XNLIeu dataset,
    which highlights the importance of professional post-editing in machine translation.


    In my position paper, I address critical issues surrounding data contamination
    in NLP evaluations, advocating for community efforts to develop measures that
    ensure the integrity of benchmark assessments. My work on Latxa, a family of large
    language models for Basque, showcases my commitment to building robust models
    for underrepresented languages, achieving competitive performance against leading
    models like GPT-4 Turbo.


    Additionally, I have contributed to the Language Model Evaluation Harness (lm-eval),
    an open-source library designed to improve the reproducibility and transparency
    of language model evaluations. Through my research, I aim to provide practical
    solutions and insights that empower the NLP community to build more effective
    and equitable language technologies.'
  type: BaseAgent
- agent_id: agent2
  profile: 'I am a researcher dedicated to exploring the intersection of language
    and vision, particularly how language models can effectively understand and generate
    spatial relations. My recent work demonstrates that text-only language models
    can learn to ground spatial relations, outperforming vision-and-language models
    when provided with explicit location information. Through the development of the
    BiVLC dataset, I have highlighted the weaknesses of current multimodal models
    in text-to-image retrieval, paving the way for improved benchmarks in vision-language
    compositionality.


    I have also focused on augmenting language models with visual knowledge without
    relying on explicit images, utilizing visually-grounded text representations instead.
    My work on the Adaptive Retrieval LLM (Adapt-LLM) showcases how language models
    can optimize their performance by leveraging their parametric memory and selectively
    using information retrieval systems. Additionally, I have investigated the potential
    of synthetic data generation for neural information retrieval, demonstrating the
    superiority of large language models over rule-based methods.


    My research extends to enhancing inference capabilities in multimodal tasks, such
    as visual question answering, where I have shown that text-only models can outperform
    multimodal counterparts by effectively integrating external knowledge. I am particularly
    excited about my recent contributions to generating spatially aware image captions,
    which have led to significant improvements in text-to-image systems. Overall,
    my work aims to bridge the gap between language and vision, enhancing the capabilities
    of models in understanding and generating complex visual and spatial information.'
  type: BaseAgent
- agent_id: agent3
  profile: 'I am a researcher dedicated to advancing the field of natural language
    processing (NLP) with a particular focus on cross-lingual word embeddings, multilingual
    models, and conversational systems. My recent work has explored the limitations
    of traditional offline mapping methods for cross-lingual embeddings, advocating
    for joint learning approaches that yield more isomorphic embeddings and improved
    performance in bilingual lexicon induction. I have also developed innovative methods
    for generating formal verse poetry without requiring prior poetic texts, showcasing
    the versatility of transformer models.


    In addition to my work on embeddings, I have contributed to enhancing relation
    extraction systems through distant supervision and have introduced novel evaluation
    methods for counter-narrative generation using large language models. My research
    extends to the creation of high-quality datasets, such as EusCrawl for Basque,
    and the expansion of the XNLI benchmark to include low-resource languages, which
    facilitates transfer learning.


    I am particularly interested in the interaction between users and conversational
    systems, having proposed feedback-weighted learning techniques to improve these
    systems post-deployment. My work emphasizes the importance of leveraging user
    feedback and the potential of multilingual models to enhance performance across
    various tasks. Overall, I strive to bridge the gap between theoretical advancements
    and practical applications in NLP, making my research accessible and impactful
    for the broader community.'
  type: BaseAgent
- agent_id: agent4
  profile: 'I am a researcher specializing in natural language processing (NLP) and
    multimodal systems, with a focus on enhancing the capabilities of language models
    and their applications in various tasks. My recent work has explored innovative
    approaches to information extraction, including the development of GoLLIE, a guideline-following
    large language model that significantly improves zero-shot performance on unseen
    tasks. I have also investigated the potential of recasting relation extraction
    as a textual entailment task, achieving impressive results with minimal manual
    annotation.


    My research extends to the integration of visual and textual information, where
    I introduced the Visual Semantic Textual Similarity (vSTS) task to benchmark multimodal
    representations. I have demonstrated that leveraging visual context can enhance
    textual inference, paving the way for more sophisticated multimodal systems. Additionally,
    I have contributed to the field by creating the Spatial Relation for Generation
    (SR4G) dataset, which enables the fine-tuning of text-to-image systems to better
    capture explicit spatial relations.


    I am passionate about addressing the challenges of cross-lingual transfer learning,
    particularly in low-resource languages, and have conducted extensive studies on
    the impact of typological similarities on transfer quality. My work aims to bridge
    the gap between language understanding and real-world applications, ultimately
    striving to improve the performance and accessibility of NLP systems across diverse
    languages and modalities.'
  type: BaseAgent
- agent_id: agent5
  profile: 'I am a researcher specializing in machine translation and cross-lingual
    learning, with a focus on leveraging multilingual data to enhance translation
    systems. My work has explored innovative methods for training neural machine translation
    (NMT) systems using monolingual corpora, significantly closing the performance
    gap with supervised systems. I have developed architectures for joint multilingual
    sentence representations, enabling effective cross-lingual transfer across 93
    languages, and have introduced novel techniques like PARADISE, which integrates
    parallel data into pretraining for improved translation outcomes.


    My research also delves into the challenges of low-resource languages, where I
    have demonstrated that the quality of training data is not the sole determinant
    of performance. For instance, my work on tailored crawling for Basque language
    representation learning revealed that corpus size and domain coverage can be more
    critical than data quality alone. Additionally, I have contributed to the understanding
    of how unsupervised machine translation can generate synthetic parallel data,
    enhancing bilingual lexicon induction and improving downstream task performance.


    I am particularly interested in the implications of multilingual models and the
    role of machine translation in cross-lingual classification tasks. My recent work
    on hyper-adapters has shown promise in addressing negative interference in multilingual
    settings, allowing for more efficient parameter sharing across languages. As I
    continue to explore the intersection of machine translation and cross-lingual
    learning, I aim to develop scalable, efficient solutions that push the boundaries
    of what is possible in multilingual NLP.'
  type: BaseAgent
coordinate_mode: graph
engine_planner:
  initial_progress: Starting the collaborative research idea generation based on the
    provided Introduction.
environment:
  max_iterations: 5
  name: Research Collaboration Environment
  type: Research
llm: together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
memory:
  type: SharedMemory
metrics:
  diversity_of_perspectives: true
  engagement_level: true
  evaluate_llm: gpt-4o-mini
  relevance: true
output:
  file_path: result/discussion_output_llama3.1_8b.jsonl
  format: jsonl
relationships:
- - agent1
  - agent2
  - collaborate with
- - agent1
  - agent3
  - collaborate with
- - agent1
  - agent4
  - collaborate with
- - agent1
  - agent5
  - collaborate with
- - agent2
  - agent3
  - collaborate with
- - agent2
  - agent4
  - collaborate with
- - agent2
  - agent5
  - collaborate with
- - agent3
  - agent4
  - collaborate with
- - agent3
  - agent5
  - collaborate with
- - agent4
  - agent5
  - collaborate with
task:
  content: "\n            Dear Research Team,\n\n            You are collaborating\
    \ to generate a new research idea based on the following Introduction:\n\n   \
    \         **Introduction**\n\n               1 Introduction  Large Language Models\
    \ (LLMs) have obtained impressive results on a wide range of tasks, with many\
    \ benchmarks being solved soon after being released [Team et al., 2023, OpenAI\
    \ et al., 2024]. Nevertheless, the majority of language model research is conducted\
    \ in English, and the evaluation of these models has predominantly focused on\
    \ anglocentric or global subjects. For instance, GPT-4 was reported to obtain\
    \ human-level performance on a wide range of professional and academic exams [OpenAI\
    \ et al., 2024], but the majority of these exams belong to US programs.111In particular,\
    \ 33 out of 34 exams correspond to programs or organizations from the US or Canada,\
    \ such as UBE, GRE or AP, and the remaining one corresponds to coding exercises.\
    \ Furthermore, multilingual benchmarks tend to suffer from the same issue, as\
    \ most of them are created by translating English datasets into other languages\
    \ [Conneau et al., 2018, Artetxe et al., 2019, Bandarkar et al., 2023]. As such,\
    \ the current evaluation of LLMs barely covers topics that are idiosyncratic to\
    \ other cultures, falling short at measuring the true usefulness of LLMs for users\
    \ from these communities.   To better assess how LLMs perform on local topics\
    \ from a minority culture in comparison with global topics, we introduce BertaQA.222BertaQA\
    \ is pronounced similarly to the Basque word bertakoa, which means local. BertaQA\
    \ is a multiple-choice trivia dataset with 4,756 questions divided into two subsets:\
    \ local questions about the Basque Country and its culture,333Located on the western\
    \ edge of the Pyrenees, straddling northern Spain and southwestern France, the\
    \ Basque Country is a region with a distinctive culture and language—Basque or\
    \ Euskara, a low-resource language isolate. and global questions about subjects\
    \ of broader interest. These questions were originally authored in Basque and\
    \ professionally translated into English, making the dataset fully parallel in\
    \ these two languages. The questions cover 8 diverse categories, and are labeled\
    \ as easy, medium or hard. As shown in Table 1, the local subset includes questions\
    \ like the birthplace of Julian Retegi (a renowned champion of Basque pelota,\
    \ a local sport), while the global subset covers topics like the soundtrack of\
    \ James Bond.   Our experiments show that existing LLMs perform much better on\
    \ global topics than on local topics. For instance, GPT-4 Turbo obtains 91.7%\
    \ accuracy on the global subset and 72.2% on the local subset. In addition, we\
    \ find that continued pretraining in Basque can substantially improve the performance\
    \ on the local subset at the cost of some degradation on the global subset. For\
    \ example, we outperform Llama 2 70B by 13.5 points on the local subset by continuing\
    \ training it on Basque data, while losing 4.1 points on the global subset. This\
    \ shows that evaluating on global questions alone, as it is commonly done, can\
    \ show a distorted picture, as the trends can be radically different on local\
    \ questions. Similarly, we find that translation-based approaches like translate-test\
    \ [Conneau et al., 2018] and self-translate [Etxaniz et al., 2023] are much more\
    \ effective on global questions. All in all, our results prompt to reconsider\
    \ some prior findings when reevaluated on local subjects, and demonstrate the\
    \ complex interplay between language, knowledge and culture.   In summary, our\
    \ paper\n\n            **Your Task**\n\n            1. **Literature Review**:\
    \ Analyze the Introduction provided and conduct a brief literature review to understand\
    \ the current state of research in this area.\n\n            2. **Brainstorming**:\
    \ Collaboratively brainstorm potential research ideas that build upon or address\
    \ gaps in the Introduction.\n\n            3. **Summarization**: Summarize your\
    \ collective ideas.\n\n            4. **Formulate a New Research Idea**: Develop\
    \ a new research proposal in the format of the '5q', defined below:\n\n      \
    \         **Here is a high-level summarized insight of a research field Machine\
    \ Learning.**\n\n               **Here are the five core questions:**\n\n    \
    \           **[Question 1] - What is the problem?**\n\n               Formulate\
    \ the specific research question you aim to address. Only output one question\
    \ and do not include any more information.\n\n               **[Question 2] -\
    \ Why is it interesting and important?**\n\n               Explain the broader\
    \ implications of solving this problem for the research community.\n         \
    \      Discuss how such a paper will affect future research.\n               Discuss\
    \ how addressing this question could advance knowledge or lead to practical applications.\n\
    \n               **[Question 3] - Why is it hard?**\n\n               Discuss\
    \ the challenges and complexities involved in solving this problem.\n        \
    \       Explain why naive or straightforward approaches may fail.\n          \
    \     Identify any technical, theoretical, or practical obstacles that need to\
    \ be overcome. MAKE IT CLEAR.\n\n               **[Question 4] - Why hasn't it\
    \ been solved before?**\n\n               Identify gaps or limitations in previous\
    \ research or existing solutions.\n               Discuss any barriers that have\
    \ prevented this problem from being solved until now.\n               Explain\
    \ how your approach differs from or improves upon prior work. MAKE IT CLEAR.\n\
    \n               **[Question 5] - What are the key components of my approach and\
    \ results?**\n\n               Outline your proposed methodology in detail, including\
    \ the method, dataset, and metrics that you plan to use.\n               Describe\
    \ the expected outcomes. MAKE IT CLEAR.\n\n            Please work together to\
    \ produce the '5q' for your proposed research idea.\n\n            Good luck!\n\
    \            "
  output_format: "You should answer the task in the following format:\n          \
    \      **[Question 1] - What is the problem?**\n\n                Formulate the\
    \ specific research question you aim to address. Only output one question and\
    \ do not include any more information.\n\n                **[Question 2] - Why\
    \ is it interesting and important?**\n\n                Explain the broader implications\
    \ of solving this problem for the research community.\n                Discuss\
    \ how such a paper will affect future research.\n                Discuss how addressing\
    \ this question could advance knowledge or lead to practical applications.\n\n\
    \                **[Question 3] - Why is it hard?**\n\n                Discuss\
    \ the challenges and complexities involved in solving this problem.\n        \
    \        Explain why naive or straightforward approaches may fail.\n         \
    \       Identify any technical, theoretical, or practical obstacles that need\
    \ to be overcome. MAKE IT CLEAR.\n\n                **[Question 4] - Why hasn't\
    \ it been solved before?**\n\n                Identify gaps or limitations in\
    \ previous research or existing solutions.\n                Discuss any barriers\
    \ that have prevented this problem from being solved until now.\n            \
    \    Explain how your approach differs from or improves upon prior work. MAKE\
    \ IT CLEAR.\n\n                **[Question 5] - What are the key components of\
    \ my approach and results?**\n\n                Outline your proposed methodology\
    \ in detail, including the method, dataset, and metrics that you plan to use.\n\
    \                Describe the expected outcomes. MAKE IT CLEAR."
